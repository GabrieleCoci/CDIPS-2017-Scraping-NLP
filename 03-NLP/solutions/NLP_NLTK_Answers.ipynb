{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Classifying Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tokenization (and basic bag-of-words features)\n",
    "\n",
    "Here is the code we went over at the start, to get started classifying documents by sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "\n",
    "random.seed(100)\n",
    "np.random.seed(100)\n",
    "\n",
    "# Read in a list of document (wordlist, category) tuples, and shuffle\n",
    "docs_tuples = [(movie_reviews.words(fileid), category)\n",
    "               for category in movie_reviews.categories()\n",
    "               for fileid in movie_reviews.fileids(category)[:200]]\n",
    "random.shuffle(docs_tuples)\n",
    "\n",
    "# Create a list of the most frequent words in the entire corpus\n",
    "movie_words = [word.lower() for (wordlist, cat) in docs_tuples for word in wordlist]\n",
    "all_wordfreqs = nltk.FreqDist(movie_words)\n",
    "top_wordfreqs = all_wordfreqs.most_common()[:1000]\n",
    "feature_words = [x[0] for x in top_wordfreqs]\n",
    "\n",
    "# Define a function to extract features of the form containts(word) for each document\n",
    "def document_features(doc_toks):\n",
    "    document_words = set(doc_toks)\n",
    "    features = {}\n",
    "    for word in feature_words:\n",
    "        features['contains({})'.format(word)] = 1 if word in document_words else 0\n",
    "    return features\n",
    "\n",
    "# Create feature sets of document (features, category) tuples\n",
    "featuresets = [(document_features(wordlist), cat) for (wordlist, cat) in docs_tuples]\n",
    "\n",
    "# Separate train and test sets, train the classifier, print accuracy and best features\n",
    "train_set, test_set = featuresets[:-100], featuresets[-100:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We left the first part of the code the same as above, but created a new list of most common adjectives as our feature words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_tuples = [(movie_reviews.sents(fileid), category, movie_reviews.words(fileid))\n",
    "               for category in movie_reviews.categories()\n",
    "               for fileid in movie_reviews.fileids(category)[:200]]\n",
    "\n",
    "random.shuffle(docs_tuples)\n",
    "\n",
    "# Create a list of the most frequent words in the entire corpus\n",
    "movie_sents = [t[0] for sublist in docs_tuples for t in sublist]\n",
    "movie_tokstags = nltk.pos_tag_sents(movie_sents)\n",
    "movie_adjs = [t[0] for sent in movie_tokstags for t in sent if re.match('JJ', t[1])]\n",
    "all_adjfreqs = FreqDist(movie_adjs)\n",
    "top_adjfreqs = all_adjfreqs.most_common()[:1000]\n",
    "feature_words = [x[0] for x in top_adjfreqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we left the document_features() function and remaining code the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features of the form containts(word) for each document\n",
    "def document_features(doc_toks):\n",
    "    document_words = set(doc_toks)\n",
    "    features = {}\n",
    "    for word in feature_words:\n",
    "        features['contains({})'.format(word)] = 1 if word in document_words else 0\n",
    "    return features\n",
    "\n",
    "\n",
    "# Create feature sets of document (features, category) tuples\n",
    "featuresets = [(document_features(wordlist), cat) for (sent_list, cat, wordlist) in docs_tuples]\n",
    "\n",
    "# Separate train and test sets, train the classifier, print accuracy and best features\n",
    "train_set, test_set = featuresets[:-100], featuresets[-100:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
